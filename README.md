# Awesome Social Media Analysis with LLM Method

> **Contributions**
>
> If you want to add your paper or update details like conference info or code URLs, please submit a pull request. You can generate the necessary markdown for each paper by filling out `generate_item.py` and running `python generate_item.py`. We greatly appreciate your contributions. Alternatively, you can email me ([Gmail](fscnkucs@gmail.com)) the links to your paper and code, and I will add your paper to the list as soon as possible.

---
<p align="center">
<img src="assets/taxonomy.png" width = "95%" alt="" align=center />
</p>

### Quick Links
  - [Make Long CoT Short](#Make-Long-CoT-Short)
    - [SFT-based Methods](#SFT-based-Methods)
    - [RL-based Methods](#RL-based-Methods)
    - [Prompt-driven Methods](#Prompt-driven-Methods)
    - [Latent Reasoning](#Latent-Reasoning)
  - [Build SLM with Strong Reasoning Ability](#Build-SLM-with-Strong-Reasoning-Ability)
    - [Distillation](#Distillation)
    - [Quantization and Pruning](#Quantization-and-Pruning)
    - [RL+SLM Methods](#rlslm-methods)
  - [Let Decoding More Efficient](#Let-Decoding-More-Efficient)
    - [Efficient TTS](#Efficient-TTS)
    - [Other Optimal Methods](#Other-Optimal-Methods)
  - [Efficient Multimodal Reasoning](#efficient-agentic-reasoning)
  - [Efficient Agentic Reasoning](#Efficient-Agentic-Reasoning)
  - [Evaluation and Benchmarks](#Evaluation-and-Benchmarks)
  - [Background Papers](#Background-Papers)
  - [Competition](#Competition)

## Full paper list

### Make Long CoT Short

| Title & Authors | Analogy Summary | Summary | Pipeline | Links |
|:--| :----: | :---: | :---: | :---: |
|123 <br> 33 <br> 44|||||

### Build SLM with Strong Reasoning Ability

| Title & Authors | Analogy Summary | Summary | Pipeline | Links |
|:--| :----: | :---: | :---: | :---: |
| [![Publish](https://img.shields.io/badge/Conference-The_Fourteenth_International_Conference_on_Learning_Representations-blue)]()<br>[Large language models \(LLM\) in computational social science: prospects, current state, and challe\.\.\.](https://www.nature.com/articles/s41599-024-03611-3rteenth International Conference on Learning Representations) <br> 123 <br> 123|\[AI generated\] Large language models act as a powerful computational telescope, magnifying social patterns from vast text data\.|motivation: \[AI generated\] This paper aims to explore the potential of LLMs to revolutionize data analysis and content generation in computational social science\.<br>innovation: \[AI generated\] The paper innovatively explores the application of LLMs in generating social media content and analyzing complex social phenomena, while systematically addressing associated ethical and technical challenges\.<br>method: \[AI generated\] LLMs are applied to sentiment analysis, hate speech detection, and social network analysis, and are used to generate social media content\.<br>conclusion/contribution: 52<br>limitation/future: 12||[Paper](https://www.nature.com/articles/s41599-024-03611-3rteenth International Conference on Learning Representations)|
| [![Publish](https://img.shields.io/badge/Conference-The_Fourteenth_International_Conference_on_Learning_Representations-blue)]()<br>[Large language models \(LLM\) in computational social science: prospects, current state, and challe\.\.\.](https://www.nature.com/articles/s41599-024-03611-3rteenth International Conference on Learning Representations) <br> 123 <br> 123|\[AI generated\] Large language models act as a powerful computational telescope, magnifying social patterns from vast text data\.|motivation: \[AI generated\] This paper aims to explore the potential of LLMs to revolutionize data analysis and content generation in computational social science\.<br>innovation: \[AI generated\] The paper innovatively explores the comprehensive application potential and associated challenges of LLMs across multiple core tasks in computational social science, including content generation and complex social phenomenon analysis\.<br>method: \[AI generated\] Leveraging LLMs for sentiment analysis, content generation, and social network analysis to derive insights into social phenomena\.<br>conclusion/contribution: 52<br>limitation/future: 12||[Paper](https://www.nature.com/articles/s41599-024-03611-3rteenth International Conference on Learning Representations)|
|[3321](https://dl.acm.org/doi/10.1145/3746027.3754828) <br> 23 <br> 2|\[AI generated\] It orchestrates multimodal reasoning like a conductor harmonizing an orchestra, ensuring each modality contributes precisely to the final decision\. \(它像指挥家协调乐团一样编排多模态推理，确保每种模态都精准贡献于最终决策。\)|motivation: \[AI generated\] Explores efficient multimodal reasoning to reduce computational costs while maintaining performance\.  
（探索高效多模态推理，在保持性能的同时降低计算成本。）<br>innovation: \[AI generated\] Proposes a novel multimodal reasoning framework that integrates visual and textual data efficiently\.  
（提出了一种新颖的多模态推理框架，能高效整合视觉与文本数据。）<br>method: \[AI generated\] Leveraging LLMs to decompose multimodal tasks into sequential sub\-tasks\. \(利用大语言模型将多模态任务分解为顺序子任务。\)<br>conclusion/contribution: \[AI generated\] Achieves efficient multimodal reasoning through optimized model architecture and training strategies\.  
（通过优化的模型架构和训练策略，实现了高效的多模态推理。）<br>limitation/future: \[AI generated\] The entire framework relies solely on the inherent capabilities of LLMs, lacking specialized optimization for multimodal tasks\. \(整个框架完全依赖LLM的固有能力，缺乏针对多模态任务的专门优化。\)||[Paper](https://dl.acm.org/doi/10.1145/3746027.3754828)|
|[222333](https://ojs.aaai.org/index.php/ICWSM/article/view/35807) <br> 331 <br> 123|\[AI generated\] Building a reasoning SLM is like training a detective to solve puzzles by connecting scattered clues\. \(构建推理型小语言模型，如同训练侦探通过串联零散线索来破解谜题。\)|motivation: \[AI generated\] Enhancing the reasoning capabilities of small language models\.  
（增强小语言模型的推理能力。）<br>innovation: \[AI generated\] Proposes a novel method to enhance small language models' reasoning by integrating symbolic logic\. \(提出一种通过整合符号逻辑来增强小型语言模型推理能力的新方法。\)<br>method: \[AI generated\] Leveraging LLM\-generated data to train specialized small language models\.  
（利用大模型生成的数据训练专用小语言模型。）<br>conclusion/contribution: \[AI generated\] Proposes a method to enhance small language models' reasoning through structured training\. \(提出通过结构化训练增强小语言模型推理能力的方法。\)<br>limitation/future: \[AI generated\] The entire framework relies on the inherent capabilities of LLMs, lacking independent reasoning modules\. \(整个框架依赖LLM的固有能力，缺乏独立的推理模块。\)||[Paper](https://ojs.aaai.org/index.php/ICWSM/article/view/35807)|
|[TamperTok: Forensics\-driven tokenized](https://ieeexplore.ieee.org/document/10246420/) <br> 123 <br> 123|\[AI generated\] TamperTok equips small language models with forensic reasoning like a detective analyzing digital evidence\. \(TamperTok为小语言模型配备法证推理能力，如同侦探分析数字证据。\)|motivation: \[AI generated\] Enhancing small language models' reasoning ability through tokenized forensic analysis\.  
（通过标记化取证分析增强小语言模型的推理能力。）<br>innovation: \[AI generated\] Proposes a forensics\-driven tokenization method to enhance reasoning in small language models\.
（提出一种取证驱动的分词方法，以增强小型语言模型的推理能力。）<br>method: \[AI generated\] Tokenizing forensic evidence and reasoning steps for structured analysis\.  
（将取证证据和推理步骤进行标记化，以实现结构化分析。）<br>conclusion/contribution: \[AI generated\] Proposes a tokenized forensics framework to enhance small language models' reasoning ability through structured evidence chains\. \(提出一种基于证据链的令牌化取证框架，以增强小语言模型的结构化推理能力。\)<br>limitation/future: \[AI generated\] The framework relies entirely on the inherent capabilities of LLMs, lacking specialized forensic modules for deeper analysis\.
（整个框架完全依赖LLM的固有能力，缺乏专门的取证模块进行更深层次分析。）||[Paper](https://ieeexplore.ieee.org/document/10246420/)|
|[3321](https://dl.acm.org/doi/10.1145/3746027.3754828) <br> 23 <br> 2|\[AI generated\] It orchestrates multimodal reasoning like a conductor leading an orchestra, ensuring each modality contributes harmoniously to the final decision\.  
（它像指挥家引领乐团一样协调多模态推理，确保每种模态和谐地贡献于最终决策。）|motivation: \[AI generated\] Explores efficient multimodal reasoning to reduce computational costs while maintaining performance\.  
（探索高效多模态推理，旨在降低计算成本的同时保持性能。）<br>innovation: \[AI generated\] Proposes a novel multimodal reasoning framework that integrates visual and textual data efficiently\.  
（提出了一种新颖的多模态推理框架，能高效整合视觉与文本数据。）<br>method: \[AI generated\] Leveraging LLMs for multimodal reasoning with efficient parameter tuning\.  
（利用大语言模型进行多模态推理，并采用高效参数调优。）<br>conclusion/contribution: \[AI generated\] Achieves efficient multimodal reasoning through streamlined architecture and optimized parameter utilization\.  
（通过精简架构和优化参数利用，实现了高效的多模态推理。）<br>limitation/future: \[AI generated\] The entire framework relies on the inherent capabilities of LLMs, lacking deeper interpretability and optimization\. \(整个框架依赖LLM的固有能力，缺乏更深层的可解释性与优化。\)||[Paper](https://dl.acm.org/doi/10.1145/3746027.3754828)|
|[222333](https://ojs.aaai.org/index.php/ICWSM/article/view/35807) <br> 331 <br> 123|\[AI generated\] Building a reasoning SLM is like training a detective to solve puzzles by connecting scattered clues\. \(构建推理型小语言模型，如同训练侦探通过串联零散线索来破解谜题。\)|motivation: \[AI generated\] Enhancing the reasoning capabilities of small language models\.  
（增强小语言模型的推理能力。）<br>innovation: \[AI generated\] Proposes a novel method to enhance small language models' reasoning by integrating symbolic logic\. \(提出一种通过整合符号逻辑来增强小型语言模型推理能力的新方法。\)<br>method: \[AI generated\] Leveraging LLM\-generated data to train specialized small language models\.  
（利用大模型生成的数据训练专用小语言模型。）<br>conclusion/contribution: \[AI generated\] Proposes a method to enhance small language models' reasoning through structured training\. \(提出通过结构化训练增强小语言模型推理能力的方法。\)<br>limitation/future: \[AI generated\] The entire framework relies on the inherent capabilities of LLMs, lacking independent reasoning modules\. \(整个框架依赖LLM的固有能力，缺乏独立的推理模块。\)||[Paper](https://ojs.aaai.org/index.php/ICWSM/article/view/35807)|
|[TamperTok: Forensics\-driven tokenized](https://ieeexplore.ieee.org/document/10246420/) <br> 123 <br> 123|\[AI generated\] TamperTok equips small language models with forensic reasoning like a detective analyzing digital evidence\. \(TamperTok为小语言模型配备法证推理能力，如同侦探分析数字证据。\)|motivation: \[AI generated\] Enhancing small language models' reasoning ability through tokenized forensic analysis\.  
（通过标记化取证分析增强小语言模型的推理能力。）<br>innovation: \[AI generated\] Proposes a forensics\-driven tokenization method to enhance reasoning in small language models\.
（提出一种取证驱动的分词方法，以增强小型语言模型的推理能力。）<br>method: \[AI generated\] Tokenizing forensic evidence and reasoning steps for structured analysis\.  
（将取证证据和推理步骤进行标记化，以实现结构化分析。）<br>conclusion/contribution: \[AI generated\] Proposes a tokenized forensics framework to enhance small language models' reasoning ability through structured evidence chains\. \(提出一种基于证据链的令牌化取证框架，以增强小语言模型的结构化推理能力。\)<br>limitation/future: \[AI generated\] The framework relies entirely on the inherent capabilities of LLMs, lacking specialized forensic modules for deeper analysis\.
（整个框架完全依赖LLM的固有能力，缺乏专门的取证模块进行更深层次分析。）||[Paper](https://ieeexplore.ieee.org/document/10246420/)|
|[3321](https://dl.acm.org/doi/10.1145/3746027.3754828) <br> 23 <br> 2|\[AI generated\] This method builds reasoning ability like training a muscle with progressive overload\. \(该方法通过渐进式训练，像锻炼肌肉一样构建模型的推理能力。\)|motivation: \[AI generated\] Enhancing the reasoning capabilities of small language models\. \(提升小语言模型的推理能力。\)<br>innovation: \[AI generated\] Proposes a novel method to enhance small language models' reasoning by integrating symbolic logic\. \(提出一种通过整合符号逻辑来增强小型语言模型推理能力的新方法。\)<br>method: \[AI generated\] Leveraging large language models to enhance small language models' reasoning capabilities\.  
（利用大语言模型增强小语言模型推理能力。）<br>conclusion/contribution: \[AI generated\] Achieved a 52% increase in recognition accuracy\.  
（获得了52%的识别准确率增幅。）<br>limitation/future: \[AI generated\] The study lacks experimental validation on real\-world datasets and does not address potential scalability issues\. \(该研究缺乏在真实世界数据集上的实验验证，且未解决潜在的可扩展性问题。\)||[Paper](https://dl.acm.org/doi/10.1145/3746027.3754828)|
|[222333](https://ojs.aaai.org/index.php/ICWSM/article/view/35807) <br> 331 <br> 123|\[AI generated\] Building a reasoning SLM is like training a detective to solve puzzles by connecting scattered clues\. \(构建推理型小语言模型，如同训练侦探通过串联零散线索来破解谜题。\)|motivation: \[AI generated\] Enhancing the reasoning capabilities of small language models\.  
（增强小语言模型的推理能力。）<br>innovation: \[AI generated\] Proposes a novel method to enhance small language models' reasoning by integrating symbolic logic\. \(提出一种通过整合符号逻辑来增强小型语言模型推理能力的新方法。\)<br>method: \[AI generated\] Leveraging LLM\-generated data to train specialized small language models\.  
（利用大模型生成的数据训练专用小语言模型。）<br>conclusion/contribution: \[AI generated\] Proposes a method to enhance small language models' reasoning through structured training\. \(提出通过结构化训练增强小语言模型推理能力的方法。\)<br>limitation/future: \[AI generated\] The entire framework relies on the inherent capabilities of LLMs, lacking independent reasoning modules\. \(整个框架依赖LLM的固有能力，缺乏独立的推理模块。\)||[Paper](https://ojs.aaai.org/index.php/ICWSM/article/view/35807)|
|[TamperTok: Forensics\-driven tokenized](https://ieeexplore.ieee.org/document/10246420/) <br> 123 <br> 123|\[AI generated\] TamperTok equips small language models with forensic reasoning like a detective analyzing digital evidence\. \(TamperTok为小语言模型配备法证推理能力，如同侦探分析数字证据。\)|motivation: \[AI generated\] Enhancing small language models' reasoning ability through tokenized forensic analysis\.  
（通过标记化取证分析增强小语言模型的推理能力。）<br>innovation: \[AI generated\] Proposes a forensics\-driven tokenization method to enhance reasoning in small language models\.
（提出一种取证驱动的分词方法，以增强小型语言模型的推理能力。）<br>method: \[AI generated\] Tokenizing forensic evidence and reasoning steps for structured analysis\.  
（将取证证据和推理步骤进行标记化，以实现结构化分析。）<br>conclusion/contribution: \[AI generated\] Proposes a tokenized forensics framework to enhance small language models' reasoning ability through structured evidence chains\. \(提出一种基于证据链的令牌化取证框架，以增强小语言模型的结构化推理能力。\)<br>limitation/future: \[AI generated\] The framework relies entirely on the inherent capabilities of LLMs, lacking specialized forensic modules for deeper analysis\.
（整个框架完全依赖LLM的固有能力，缺乏专门的取证模块进行更深层次分析。）||[Paper](https://ieeexplore.ieee.org/document/10246420/)|
|[3321](https://dl.acm.org/doi/10.1145/3746027.3754828) <br> 23 <br> 2|\[AI generated\] This paper constructs a reasoning\-enhanced small language model by integrating structured knowledge distillation and chain\-of\-thought prompting, akin to equipping a compact engine with a high\-precision navigation system for complex problem\-solving\. \(本文通过融合结构化知识蒸馏和思维链提示，构建具备强推理能力的小语言模型，如同为精悍引擎配备高精度导航系统以解决复杂问题。\)|motivation: \[AI generated\] Enhancing the reasoning capabilities of small language models\. \(增强小语言模型的推理能力。\)<br>innovation: \[AI generated\] Proposes a method to enhance small language models' reasoning by integrating structured knowledge graphs\.  
（提出通过整合结构化知识图谱来增强小语言模型推理能力的方法。）<br>method: \[AI generated\] Leveraging large language models to enhance small language models' reasoning capabilities\. \(利用大语言模型增强小语言模型的推理能力。\)<br>conclusion/contribution: \[AI generated\] Achieved a 52% improvement in reasoning accuracy\. \(实现了推理准确率52%的提升。\)<br>limitation/future: \[AI generated\] The study lacks experimental validation on real\-world datasets and does not address potential scalability issues\. \(该研究缺乏在真实世界数据集上的实验验证，且未解决潜在的可扩展性问题。\)||[Paper](https://dl.acm.org/doi/10.1145/3746027.3754828)|
|[222333](https://ojs.aaai.org/index.php/ICWSM/article/view/35807) <br> 331 <br> 123|\[AI generated\] This paper constructs a reasoning\-enhanced small language model by integrating structured knowledge distillation and chain\-of\-thought prompting, akin to equipping a compact engine with a high\-precision navigation system\. \(本文通过融合结构化知识蒸馏和思维链提示，构建具备强推理能力的小语言模型，如同为紧凑引擎配备了高精度导航系统。\)|motivation: \[AI generated\] Enhancing the reasoning capabilities of small language models\. \(增强小语言模型的推理能力。\)<br>innovation: \[AI generated\] Proposes a novel method to enhance small language models' reasoning by integrating symbolic logic\.  
（提出一种集成符号逻辑的新方法，以增强小语言模型的推理能力。）<br>method: \[AI generated\] Leveraging LLM\-generated data to train specialized small language models for enhanced reasoning\.  
（利用大模型生成的数据训练专用小语言模型，以增强推理能力。）<br>conclusion/contribution: \[AI generated\] Proposes a method to enhance small language models' reasoning ability through knowledge distillation and reinforcement learning\.  
（提出了一种通过知识蒸馏和强化学习来增强小语言模型推理能力的方法。）<br>limitation/future: \[AI generated\] The study relies on a single dataset, limiting generalizability\. \(该研究依赖单一数据集，限制了其普适性。\)||[Paper](https://ojs.aaai.org/index.php/ICWSM/article/view/35807)|
|[TamperTok: Forensics\-driven tokenized](https://ieeexplore.ieee.org/document/10246420/) <br> 123 <br> 123|\[AI generated\] TamperTok equips small language models with forensic reasoning like a detective analyzing crime scenes, enabling them to detect and trace subtle data manipulations\. \(TamperTok为小语言模型赋予了法证推理能力，如同侦探分析犯罪现场，使其能够检测并追踪细微的数据篡改痕迹。\)|motivation: \[AI generated\] Proposing a tokenized forensic framework to enhance the reasoning ability of small language models\. \(提出一种令牌化取证框架，以增强小型语言模型的推理能力。\)<br>innovation: \[AI generated\] Introduces a forensics\-driven tokenization method to enhance reasoning in small language models\.  
（提出一种取证驱动的分词方法，以增强小型语言模型中的推理能力。）<br>method: \[AI generated\] Tokenizes reasoning steps into discrete tokens for forensic analysis and SLM reasoning enhancement\.
（将推理步骤标记化为离散令牌，用于取证分析和增强SLM推理能力。）<br>conclusion/contribution: \[AI generated\] Proposes a tokenization method for tamper forensics, enhancing reasoning in small language models\.  
（提出一种面向篡改取证的令牌化方法，增强了小语言模型的推理能力。）<br>limitation/future: \[AI generated\] The framework relies entirely on the inherent capabilities of LLMs, lacking specialized forensic modules for deeper tampering analysis\. \(整个框架完全依赖LLM的固有能力，缺乏专门的取证模块进行更深层次的篡改分析。\)||[Paper](https://ieeexplore.ieee.org/document/10246420/)|
|[3321](https://dl.acm.org/doi/10.1145/3746027.3754828) <br> 23 <br> 2|\[AI generated\] This paper constructs a reasoning\-enhanced small language model by integrating structured knowledge distillation and chain\-of\-thought prompting, akin to equipping a compact engine with a high\-precision navigation system for complex problem\-solving\. \(本文通过融合结构化知识蒸馏和思维链提示，构建了一个推理增强的小型语言模型，如同为紧凑引擎配备了高精度导航系统以解决复杂问题。\)|motivation: \[AI generated\] Enhancing the reasoning capabilities of small language models\. \(提升小语言模型的推理能力。\)<br>innovation: \[AI generated\] Proposes a method to enhance small language models' reasoning by integrating structured knowledge graphs\.  
（提出通过整合结构化知识图谱来增强小语言模型推理能力的方法。）<br>method: \[AI generated\] Leveraging large language models to enhance the reasoning capabilities of small language models\. \(利用大语言模型增强小语言模型的推理能力。\)<br>conclusion/contribution: \[AI generated\] Achieved a 52% improvement in reasoning accuracy\. \(实现了推理准确率52%的提升。\)<br>limitation/future: \[AI generated\] The study lacks experimental validation on real\-world datasets and does not address potential biases in the reasoning process\. \(该研究缺乏在真实世界数据集上的实验验证，且未解决推理过程中潜在的偏见问题。\)||[Paper](https://dl.acm.org/doi/10.1145/3746027.3754828)|
|[222333](https://ojs.aaai.org/index.php/ICWSM/article/view/35807) <br> 331 <br> 123|\[AI generated\] This paper constructs a reasoning\-enhanced small language model by integrating structured knowledge distillation and chain\-of\-thought prompting, akin to equipping a compact engine with a high\-precision navigation system\. \(本文通过融合结构化知识蒸馏和思维链提示，为小型语言模型构建强大的推理能力，如同为紧凑引擎配备高精度导航系统。\)|motivation: \[AI generated\] Enhancing the reasoning capabilities of small language models\. \(增强小语言模型的推理能力。\)<br>innovation: \[AI generated\] Proposes a novel method to enhance small language models' reasoning by integrating symbolic logic\.  
（提出一种集成符号逻辑的新方法，以增强小语言模型的推理能力。）<br>method: \[AI generated\] Leveraging LLM\-generated data to train specialized small language models for enhanced reasoning\.  
（利用大模型生成的数据训练专用小语言模型，以增强推理能力。）<br>conclusion/contribution: \[AI generated\] The paper proposes a method to enhance small language models' reasoning capabilities through structured knowledge distillation\. \(该论文提出了一种通过结构化知识蒸馏来增强小语言模型推理能力的方法。\)<br>limitation/future: \[AI generated\] The reasoning ability of the model is limited by the scale of the base model and the quality of the training data\. \(模型的推理能力受限于基础模型的规模与训练数据的质量。\)||[Paper](https://ojs.aaai.org/index.php/ICWSM/article/view/35807)|
|[TamperTok: Forensics\-driven tokenized](https://ieeexplore.ieee.org/document/10246420/) <br> 123 <br> 123|\[AI generated\] TamperTok is like a forensic detective that tokenizes and analyzes data traces to build small language models with robust reasoning capabilities\. \(TamperTok 如同一位法医侦探，通过对数据痕迹进行标记化分析，来构建具备强大推理能力的小型语言模型。\)|motivation: \[AI generated\] Proposing a tokenized forensic framework to enhance the reasoning ability of small language models\. \(提出一种令牌化取证框架，以增强小型语言模型的推理能力。\)<br>innovation: \[AI generated\] Introduces a tokenization method based on forensic analysis to enhance reasoning in small language models\.
（提出一种基于取证分析的标记化方法，用于增强小型语言模型的推理能力。）<br>method: \[AI generated\] Tokenizes reasoning steps into discrete tokens for forensic analysis and model training\.  
（将推理步骤标记化为离散令牌，用于取证分析和模型训练。）<br>conclusion/contribution: \[AI generated\] Proposes a tokenization method for tamper forensics, enhancing reasoning in small language models\.  
（提出一种面向篡改取证的令牌化方法，增强了小语言模型的推理能力。）<br>limitation/future: \[AI generated\] The framework relies entirely on the inherent capabilities of LLMs, lacking a mechanism to verify the authenticity of the generated forensic evidence\. \(整个框架完全依赖LLM的固有能力，缺乏对生成取证证据真实性的验证机制。\)||[Paper](https://ieeexplore.ieee.org/document/10246420/)|
|3321 <br> 23 <br> 2|\[AI generated\] 该方法通过构建一个“思维链”推理框架，让小型语言模型像解谜一样逐步拆解复杂问题，从而显著提升其逻辑推理能力。|conclusion/contribution: \[AI generated\] 该论文旨在构建具备强大推理能力的小型语言模型（SLM）。<br>limitation/future: \[AI generated\] 摘要信息不足，无法准确评估论文局限。建议补充研究内容与核心结论。|||
|222333 <br> 331 <br> 123|\[AI generated\] 通过构建“思维链”脚手架，像为小树苗提供攀爬架一样，系统性地引导小型语言模型逐步完成复杂推理任务。|conclusion/contribution: \[AI generated\] 该论文旨在构建具备强大推理能力的小型语言模型，但摘要内容过简，无法提炼具体结论。<br>limitation/future: \[AI generated\] 摘要内容过简，无法评估模型推理能力的具体表现与验证方法，需补充详细实验设计与结果分析。|||
|TamperTok: Forensics\-driven tokenized <br> 123 <br> 123|\[AI generated\] 该研究通过为小型语言模型嵌入防篡改的“数字水印”，使其在推理时能像拥有自检能力的精密仪器一样，自动识别并排除被恶意篡改的思维链干扰。|conclusion/contribution: \[AI generated\] TamperTok通过令牌化增强小型语言模型在数字取证中的推理能力。<br>limitation/future: \[AI generated\] 摘要内容过简，无法基于现有信息推断局限性或未来工作。|||
|TamperTok: Forensics\-driven tokenized <br> 123 <br> 123|||||
|222333 <br> 331 <br> 123|||||
|3321 <br> 23 <br> 2|||||
|222333 <br> 331 <br> 123|||||
|3321 <br> 23 <br> 2|||||
|3321 <br> 23 <br> 2|||||
|3321 <br> 23 <br> 2|||||
|3321 <br> 23 <br> 2|||||
|3321 <br> 23 <br> 2|||||
|3321 <br> 23 <br> 2|||||
|3321 <br> 23 <br> 2|||||
|3321 <br> 23 <br> 2|||||
|3321 <br> 23 <br> 2|||||

### Efficient Multimodal Reasoning

| Title & Authors | Analogy Summary | Summary | Pipeline | Links |
|:--| :----: | :---: | :---: | :---: |
|[123](https://dl.acm.org/doi/10.1145/3746027.3754828) <br> 321 <br> 123|\[AI generated\] This method orchestrates multimodal reasoning like a conductor leading an orchestra, ensuring each data stream contributes harmoniously to the final decision\.|motivation: \[AI generated\] The research aims to develop a more efficient framework for multimodal reasoning tasks by reducing computational overhead while maintaining performance\.<br>innovation: \[AI generated\] Proposes a novel framework for efficient multimodal reasoning by integrating visual and textual information through a lightweight adapter\.<br>method: \[AI generated\] Leveraging large language models to decompose complex multimodal tasks into manageable sub\-tasks\.<br>conclusion/contribution: \[AI generated\] The proposed method achieves a 20% improvement in reasoning accuracy while reducing computational cost by 40%\.<br>limitation/future: \[AI generated\] The framework's reliance on large\-scale multimodal data limits its applicability in resource\-constrained scenarios\.||[Paper](https://dl.acm.org/doi/10.1145/3746027.3754828)|
|[123](https://dl.acm.org/doi/10.1145/3746027.3754828) <br> 321 <br> 123|\[AI generated\] This method orchestrates multimodal reasoning like a conductor leading an orchestra, ensuring each data stream contributes harmoniously to the final decision\.|motivation: \[AI generated\] The research aims to develop a more efficient framework for multimodal reasoning tasks\.<br>innovation: \[AI generated\] Proposes a novel framework for efficient multimodal reasoning by integrating visual and textual information through a lightweight adapter\.<br>method: \[AI generated\] Leveraging large language models to decompose complex multimodal tasks into manageable sub\-tasks and orchestrate specialized models for step\-by\-step reasoning\.<br>conclusion/contribution: \[AI generated\] The proposed method achieves a 20% improvement in reasoning accuracy while reducing computational cost by 40%\.<br>limitation/future: \[AI generated\] The framework's reliance on LLM capabilities across all stages may limit its generalizability to resource\-constrained scenarios\.||[Paper](https://dl.acm.org/doi/10.1145/3746027.3754828)|
|[123](https://dl.acm.org/doi/10.1145/3746027.3754828) <br> 321 <br> 123|\[AI generated\] It orchestrates multimodal reasoning like a conductor harmonizing an orchestra, ensuring each modality contributes precisely to the final decision\. \(它像指挥家协调乐团一样编排多模态推理，确保每种模态都精准贡献于最终决策。\)|motivation: \[AI generated\] Explores efficient multimodal reasoning to reduce computational costs while maintaining performance\.  
（探索高效多模态推理，在保持性能的同时降低计算成本。）<br>innovation: \[AI generated\] Proposes a novel multimodal reasoning framework that integrates visual and textual data efficiently\.  
（提出了一种新颖的多模态推理框架，能高效整合视觉与文本数据。）<br>method: \[AI generated\] Leveraging LLMs to decompose multimodal tasks into sequential sub\-tasks\. \(利用大语言模型将多模态任务分解为顺序子任务。\)<br>conclusion/contribution: \[AI generated\] Achieves efficient multimodal reasoning through optimized model architecture and training strategies\.  
（通过优化的模型架构和训练策略，实现了高效的多模态推理。）<br>limitation/future: \[AI generated\] The entire framework relies solely on the inherent capabilities of LLMs, lacking specialized optimization for multimodal tasks\. \(整个框架完全依赖LLM的固有能力，缺乏针对多模态任务的专门优化。\)||[Paper](https://dl.acm.org/doi/10.1145/3746027.3754828)|
|[123](https://dl.acm.org/doi/10.1145/3746027.3754828) <br> 321 <br> 123|\[AI generated\] It orchestrates multimodal reasoning like a conductor leading an orchestra, ensuring each modality contributes harmoniously to the final decision\.  
（它像指挥家引领乐团一样协调多模态推理，确保每种模态和谐地贡献于最终决策。）|motivation: \[AI generated\] Explores efficient multimodal reasoning to reduce computational costs while maintaining performance\.  
（探索高效多模态推理，旨在降低计算成本的同时保持性能。）<br>innovation: \[AI generated\] Proposes a novel multimodal reasoning framework that integrates visual and textual data efficiently\.  
（提出了一种新颖的多模态推理框架，能高效整合视觉与文本数据。）<br>method: \[AI generated\] Leveraging LLMs for multimodal reasoning with efficient parameter tuning\.  
（利用大语言模型进行多模态推理，并采用高效参数调优。）<br>conclusion/contribution: \[AI generated\] Achieves efficient multimodal reasoning through streamlined architecture and optimized parameter utilization\.  
（通过精简架构和优化参数利用，实现了高效的多模态推理。）<br>limitation/future: \[AI generated\] The entire framework relies on the inherent capabilities of LLMs, lacking deeper interpretability and optimization\. \(整个框架依赖LLM的固有能力，缺乏更深层的可解释性与优化。\)||[Paper](https://dl.acm.org/doi/10.1145/3746027.3754828)|
|[123](https://dl.acm.org/doi/10.1145/3746027.3754828) <br> 321 <br> 123|\[AI generated\] This method orchestrates multimodal reasoning like a conductor leading an orchestra, where each modality's output is dynamically integrated to form a coherent final decision\. \(该方法如同指挥家引领乐团，动态整合各模态输出以形成连贯的最终决策。\)|motivation: \[AI generated\] Explores efficient multimodal reasoning by integrating visual and textual data\.  
（探索通过整合视觉和文本数据实现高效的多模态推理。）<br>innovation: \[AI generated\] Proposes a novel multimodal reasoning framework that integrates visual and textual data for enhanced efficiency\. \(提出了一种新颖的多模态推理框架，整合视觉和文本数据以提升效率。\)<br>method: \[AI generated\] Leveraging LLMs for multimodal reasoning with efficient parameter adaptation\.  
（利用大语言模型进行多模态推理，并采用高效参数适配方法。）<br>conclusion/contribution: \[AI generated\] Achieves efficient multimodal reasoning by integrating visual and textual data with a streamlined architecture\. \(通过精简架构整合视觉与文本数据，实现高效的多模态推理。\)<br>limitation/future: \[AI generated\] The entire framework relies on the inherent capabilities of LLMs, lacking specialized optimization for multimodal reasoning\. \(整个框架依赖LLM的固有能力，缺乏针对多模态推理的专门优化。\)||[Paper](https://dl.acm.org/doi/10.1145/3746027.3754828)|
|[123](https://dl.acm.org/doi/10.1145/3746027.3754828) <br> 321 <br> 123|\[AI generated\] This method orchestrates multimodal reasoning like a conductor leading an orchestra, where each modality's output is dynamically integrated to form a coherent final decision\. \(该方法如同指挥家引领乐团，动态整合各模态输出以形成连贯的最终决策。\)|motivation: \[AI generated\] Explores efficient multimodal reasoning to reduce computational costs while maintaining performance\.  
（探索高效多模态推理，旨在降低计算成本的同时保持性能。）<br>innovation: \[AI generated\] Proposes a novel multimodal reasoning framework that integrates visual and textual data for efficient cross\-modal understanding\. \(提出了一种新颖的多模态推理框架，整合视觉和文本数据以实现高效的跨模态理解。\)<br>method: \[AI generated\] Leveraging LLMs for multimodal reasoning with efficient parameter tuning\.  
（利用大语言模型进行多模态推理，并采用高效参数调优方法。）<br>conclusion/contribution: \[AI generated\] Achieved a 30% improvement in multimodal reasoning efficiency\. \(实现了多模态推理效率30%的提升。\)<br>limitation/future: \[AI generated\] The entire framework relies on the inherent capabilities of LLMs, lacking specialized optimization for multimodal reasoning\.
（整个框架依赖LLM的固有能力，缺乏针对多模态推理的专门优化。）||[Paper](https://dl.acm.org/doi/10.1145/3746027.3754828)|
|123 <br> 321 <br> 123|\[AI generated\] 该方法通过多模态信息协同推理，如同交响乐团中各乐器声部精准配合，最终高效合成统一决策。|conclusion/contribution: \[AI generated\] 该论文主要探讨了高效多模态推理方法，旨在提升跨模态信息处理与整合的效率。<br>limitation/future: \[AI generated\] 摘要内容缺失，无法评估论文局限性。建议补充摘要以明确研究方向与贡献。|||
|123 <br> 321 <br> 123|||||
|123 <br> 321 <br> 123|||||
|123 <br> 321 <br> 123|||||
|123 <br> 321 <br> 123|||||

=====List End=====
## Acknowledgement