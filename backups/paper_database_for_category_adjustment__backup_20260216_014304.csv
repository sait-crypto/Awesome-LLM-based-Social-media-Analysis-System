doi,title,authors,publish date,category,summary motivation,summary innovation,summary method,summary conclusion,summary limitation,paper url,project url,conference,title translation,analogy summary,pipeline figure,abstract,contributor,notes,show in readme,status,submission time,is placeholder,conflict marker,invalid fields,paper file,UID
doi,title,authors,date,category,summary_motivation,summary_innovation,summary_method,summary_conclusion,summary_limitation,paper_url,project_url,conference,title_translation,analogy_summary,pipeline_image,abstract,contributor,notes,show_in_readme,status,submission_time,is_placeholder,conflict_marker,invalid_fields,paper_file,uid
10.1609/aaai.v37i4.25614,TOT: Topology-Aware Optimal Transport for Multimodal Hate Detection,"Linhao Zhang，Li Jin，Xian Sun，Guangluan Xu，Zequn Zhang,Xiaoyu Li,Nayu Liu,Qing Liu,Shiyao Yan",2023/6/26,Hate Speech Analysis,为了解决多模态仇恨检测中因” 隐式对齐” 和” 模态鸿沟” 导致的图像和文本跨模态语义对齐难题,将OT用于特征对齐，将句子级对齐细粒化至向量级，为后续工作提供了“显式对齐+结构推理”的范式,最优传输 + 拓扑结构推理方法 TOT：CLIP 方法统一表征映射->最优传输optimal transport (OT)将隐式联系细粒化为向量级（这是一个数学计算过程，不涉及需要学习的参数）->类GNN迭代捕捉自身语义联系（类自注意力）（因为向量间距离意义明确）->残差连接,"达成了在两个有害 Meme 检测数据集（Harm-C, Harm-P）上的最先进性能；",对齐和推理仍局限于特征层面，未上升到语义单元（如事件、概念）层面，OT过程为冻结无法训练的，可以训练其参数以实现更好的对齐；对于幽默等类似隐式表达容易误判,https://ojs.aaai.org/index.php/AAAI/article/view/25614,,Proceedings of the AAAI Conference on Artificial Intelligence,[AI generated] **中文标题：** TOT：面向多模态仇恨检测的拓扑感知最优传输方法,强化恶意Meme的图像与文本之间的语义对齐，使用OT方法建立特征向量间的可解释联系,assets/1f3e4104/TOT.png,"Multimodal hate detection, which aims to identify the harmful content online such as memes, is crucial for building a wholesome internet environment. Previous work has made enlightening exploration in detecting explicit hate remarks. However, most of their approaches neglect the analysis of implicit harm, which is particularly challenging as explicit text markers and demographic visual cues are often twisted or missing. The leveraged cross-modal attention mechanisms also suffer from the distributional modality gap and lack logical interpretability. To address these 
semantic gap issues, we propose TOT: a topology-aware optimal transport framework to decipher the implicit harm in memes scenario, which formulates the cross-modal aligning problem as solutions for optimal transportation plans. Specifically, we leverage an optimal transport kernel method to capture complementary information from multiple modalities. The kernel embedding provides a non-linear transformation ability to reproduce a kernel Hilbert space (RKHS), which reflects significance for eliminating the distributional modality gap. Moreover, we perceive the topology information based on aligned representations to conduct bipartite graph path reasoning. The newly achieved state-of-the-art performance on two publicly available benchmark datasets, together with  further visual analysis, demonstrate the superiority of TOT in capturing implicit cross-modal alignment.",anonymous,最优传输OT负责回答“图片的哪个部分和文本的哪个词相关？”（实现统一且对齐的表示，从而建立跨模态的显式联系，OT方法是可解释的）。【即将CLIP生成的特征矩阵级别的对齐，细化为特征向量间的对齐，两个特征矩阵会更相像。这种显式对齐能力本质上来源于CLIP实现的隐对齐】；拓扑建模负责回答“这些相关的部分组合在一起，表达了什么更深层的含义？”（捕捉文本（图片）中互相有联系的token（patch），进行模态内的深度推理）。【这种类似GNN的方法本质上是更有层次性的自注意力机制，天然适用于处理关系型数据（图结构）】；本质上是将CLIP建立的隐式对齐细粒化为向量层级的显式对齐，进而得以使用图推理进一步学习内部联系,TRUE,,2026/1/15 20:03,FALSE,FALSE,,,1f3e4104
10.1109/TMM.2025.3581795,Flexible optimal transport with contrastive graphical modeling for multimodal hate detection,"Linhao Zhang，Li Jin，Xiaoyu Li,Xian Sun,Senior Member,IEEE,Xin Wang,Zequn Zhang,Jian Liu，Zhicong Lu，Graduate Student Member,IEEE,and Guangluan Xu",2025,Hate Speech Analysis,社媒中隐含仇恨内容检测困难，传统方法难以实现跨模态隐式对齐。,相对于同团队的TOT是改进,相对于同团队的TOT:1.OT的目标域不再是另一模态的特征，而是可学习的统一嵌入（OT引入可学习的参数，它们是两个模态各自对应的目标特征矩阵 $T_v$ 和 $T_t$）；2.引入了图对比学习损失，显式约束一致性（比较两个图的相似程度作为一个损失，之后才进行类GNN聚合（动态拓扑推理））,在Harm-C、Harm-P、MET-Meme三个数据集上取得SOTA，显著提升准确率与F1,对于幽默等类似隐式表达容易误判,https://ieeexplore.ieee.org/abstract/document/11045556,,IEEE Transactions on Multimedia,"[AI generated] **中文标题：** 基于对比图建模与灵活最优传输的多模态仇恨内容检测

**说明：**  
此翻译在准确传达原文技术核心（Flexible Optimal Transport、contrastive graphical modeling）的同时，兼顾了学术表达的简洁性与专业性。采用“灵活最优传输”以突出方法对非显式跨模态关联的适应性，并通过“对比图建模”明确其结构化表征优化机制，整体符合计算机视觉与自然语言处理交叉领域的术语规范。","[AI generated] This method bridges multimodal gaps like a flexible translator, aligning implicit hateful memes through optimal transport and contrastive graphs. [翻译]该方法通过最优传输和图对比学习，像灵活的翻译官一样弥合模态鸿沟，对齐隐含仇恨表情包。",assets/a5a4f124/FLOT1.png;assets/a5a4f124/FLOT.png,"Multimodal hate detection plays a crucial role in maintaining harmonious online environments by identifying harmful content, such as hateful memes. Although previous research has made significant progress in detecting explicit hate speech, there remains a critical gap in analyzing implicit hate, which is particularly challenging due to the absence of explicit harmful text claims or demographic visual cues. Despite the promising results based on cross-modal attention, previous methods may suffer from the distributional modality gap caused by the non-literal associations between multimodal elements, which lacks apparent alignment in implicit hateful contents. In this work, we propose a novel framework: Flexible Optimal Transport (FLOT) to capture the non-literal cross-modal alignment for multimodal hate in the context of memes. FLOT formulates the problem of cross-modal alignment as finding optimal transportation plans, which leverages a kernel method to capture complementary information from multiple modalities. The kernel embeddings reproduce a kernel Hilbert space (RKHS) to serve as a non-linear transformation of alignment, which effectively reduces the distributional modality gap with more interpretability. Moreover, we established topological structures with contrastive modeling for the aligned representations, which are optimized to achieve comprehensive alignment between different modalities, and facilitate local reasoning based on multimodal elements. Experimental results have demonstrated that our FLOT achieved state-of-the-art performance on three publicly available benchmark datasets. Furthermore, extensive qualitative analysis confirms the superior ability of FLOT in capturing implicit cross-modal alignment.",anonymous,,TRUE,,2026/1/15 20:03,FALSE,FALSE,,,a5a4f124
10.18653/v1/2024.acl-long.291,Improving Hateful Meme Detection through Retrieval-Guided Contrastive Learning,"Jingbiao Mei,Jinghong Chen,Weizhe Lin,Bill Byrne,Maarcus Tomalin",2024,Hate Speech Analysis,现有CLIP等模型对仇恨表情包的图像-文本的细微差异（如“混淆样本”）敏感度不足，导致的误判。,对于易混淆的难例（与当前样本相似度最高但标签相反的），使用**动态检索**方式单拉出来，与**伪黄金正样本**（和当前样本相似度最高的标签相同的）成对，作为正反例进行对比学习。从而解决问题,"1. 使用冻结的CLIP编码器提取图文特征；
2. 通过Faiss检索动态获取同类相似样本（伪黄金正样本）与异类相似样本（困难负样本）作为正反例；
3. 结合正反例对比损失（RGCLL）与交叉熵损失训练MLP；
4. 实现逻辑分类与KNN检索分类两种分类器，后者通过相似度加权投票进行预测。",在HatefulMemes数据集上达到 AUROC 87.0%（SOTA），超越Flamingo-80B等大型多模态模型,仇恨言论的定义具有争议性与文化依赖性；系统对 细微面部表情 识别能力有限；依赖数据标注质量，可能存在标注偏差。,https://aclanthology.org/2024.acl-long.291,,ACL 2024,"[AI generated] **中文标题：** 通过检索引导的对比学习提升仇恨表情包检测性能

**说明：** 该翻译准确传达了原标题的技术核心（检索引导的对比学习方法）与研究目标（提升仇恨表情包检测），符合计算机科学领域，特别是多模态内容分析与仇恨言论检测方向的学术表达规范，风格专业、简洁。",专题强化：难学样本单拉出来与正例进行对比学习，从而提高识别能力,assets/539503e7/RGCL.png,"Hateful memes have emerged as a significant concern on the Internet. Detecting hateful memes requires the system to jointly understand the visual and textual modalities. Our investigation reveals that the embedding space of existing CLIP-based systems lacks sensitivity to subtle differences in memes that are vital for correct hatefulness classification. We propose constructing a hatefulness-aware embedding space through retrieval-guided contrastive training. Our approach achieves state-of-the-art performance on the HatefulMemes dataset with an AUROC of 87.0, outperforming much larger fine-tuned large multimodal models. We demonstrate a retrieval-based hateful memes detection system, which is capable of identifying hatefulness based on data unseen in training. This allows developers to update the hateful memes detection system by simply adding new examples without retraining — a desirable feature for real services in the constantly evolving landscape of hateful memes on the Internet.",anonymous,,TRUE,,2026/1/15 20:03,FALSE,FALSE,,,539503e7
10.1609/icwsm.v19i1.35837,Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic Analysis of Annotators and Targets,"Tommaso Giorgi*,Lorenzo Cima*,Tiziano Fagni,Marco Avvenuti,Stefano Cresci",2025/6/7,Hate Speech Analysis,该领域需要大量人工标注，存在固有的主观性bias问题，需要系统性的研究,"The authors introduce a novel methodological framework on the Measuring Hate Speech corpus that quantifies bias through ""Intensity"" and ""Prevalence"" metrics without relying on ground truth, uniquely isolating the interplay between specific annotator profiles and target groups.
[翻译] 指标设计：提出了偏差强度（Intensity, ??）和偏差普遍性（Prevalence, ??），无需Ground Truth即可衡量相对偏差（将**其余所有标注者（Reference Group）**的共识作为基准）。<br>LLM对齐分析：评估了角色扮演LLM在“复现标注偏差”任务上的能力","Leveraging a large-scale dataset with rich demographic attributes, the methodology employs a comparative analysis using confusion matrices to measure relative labeling discrepancies between demographic groups, subsequently evaluating open-source LLMs via role-playing prompts to assess their alignment with human bias patterns.
[翻译]通过**混淆矩阵**（行代表不具备该属性，列代表具备该属性）对比特定属性群体在评价特定属性受害者时的标签差异。计算偏差强度和普遍性\n使用**prompt**引导LLM进行相同任务以对比","Quantitative analysis reveals that while human annotators exhibit significant ""in-group"" hypersensitivity and demographic-specific labeling variations, persona-based LLMs demonstrate a limited correlation with these human biases, failing to accurately mirror the complex social prejudices inherent in human data.
[翻译] 人类偏差：存在显著的“组内高敏度”（即倾向于高估针对自身群体的仇恨），受人口统计学交互影响严重（如年轻人倾向低估仇恨，老年人倾向高估）。<br>LLM表现：M表现出自身偏差，但未能有效复现人类的特定偏差（相关性极低），**欠缺对齐能力**（高估代表更敏感）","The study's limitations include data scarcity for specific minority groups which constrains statistical significance, and a reliance solely on prompting strategies without fine-tuning, which may restrict the models' capacity for deep behavioral mimicry.
[翻译] 该研究的局限性包括特定少数群体的数据稀缺限制了统计显著性，以及仅仅依赖提示策略而没有进行微调，这可能限制了模型的深度行为模仿能力。",https://ojs.aaai.org/index.php/ICWSM/article/view/35837,,ICWSM,"[AI generated] **中文标题：仇恨言论标注中的人类与大型语言模型偏见：标注者与目标群体的社会人口学分析**

**说明：**
- 该翻译准确传达了原标题的核心要素：“Human and LLM Biases”（人类与LLM偏见）、“Hate Speech Annotations”（仇恨言论标注）以及“Socio-Demographic Analysis of Annotators and Targets”（针对标注者与目标群体的社会人口学分析）。
- 采用“大型语言模型”这一通用学术",仇恨言论分析中的数据集标注如何受主观偏见影响，提示词引导的角色扮演LLM能否复刻这种偏见,assets/f81c958e/HateAnaBias.png,"The rise of online platforms exacerbated the spread of hate speech, demanding scalable and effective detection. However, the accuracy of hate speech detection systems heavily relies on human-labeled data, which is inherently susceptible to biases. While previous work has examined the issue, the interplay between the characteristics of the annotator and those of the target of the hate are still unexplored. We fill this gap by leveraging an extensive dataset with rich socio-demographic information of both annotators and targets, uncovering how human biases manifest in relation to the target's attributes. Our analysis surfaces the presence of widespread biases, which we quantitatively describe and characterize based on their intensity and prevalence, revealing marked differences. Furthermore, we compare human biases with those exhibited by persona-based LLMs. Our findings indicate that while persona-based LLMs do exhibit biases, these differ significantly from those of human annotators. Overall, our work offers new and nuanced results on human biases in hate speech annotations, as well as fresh insights into the design of AI-driven hate speech detection systems.",anonymous,"[引用句]Serving as a foundational critique within the transition from static classification to dynamic social simulation, Giorgi et al. (2025) demonstrate that although human perception of hate speech is fundamentally shaped by the interplay between annotator and target demographics, current persona-based LLMs fail to faithfully emulate these emergent sociological biases, highlighting a critical gap in the development of realistic AI agents.
[翻译] 作为从静态分类向动态社会仿真过渡过程中的一项基础性批判研究，Giorgi等人（2025）证明，尽管人类对仇恨言论的感知从根本上受标注者与目标人口统计特征交互作用的影响，但当前的基于角色的LLM无法忠实地模拟这些涌现的社会学偏差，突显了构建逼真AI智能体方面的一个关键差距。",TRUE,,2026/1/15 20:03,FALSE,FALSE,,,f81c958e
10.1609/aaai.v38i1.27788,GAMC: An Unsupervised Method for Fake News Detection Using Graph Autoencoder with Masking,"Shu Yin,Peican Zhu,Lianwei Wu,Chao Gao,Zhen Wang",2024/3/24,Misinformation Analysis,现有方法多依赖新闻内容或需大量标注数据，难以有效利用传播上下文信息。,首个结合图自编码器、掩码与对比学习的无监督假新闻检测方法，同时利用传播结构与内容信息，无需标注数据,"1. 将新闻传播建模为图（新闻节点和用户节点，边表示转发关系，节点特征来自新闻内容和用户历史贴文）；
2. 数据增强（节点特征掩码+边丢弃）（随机选取节点将其特征替换为掩码标记，随机删除部分边）构造自监督特性；
3. 图编码器（GIN）生成潜在表示；
4. 图解码器重建特征；
5. 损失函数组成（**重建损失**（使重建特征接近原始特征）+**对比损失**（来自同一个原始图的两个增强图重建后应尽量相似））训练。",在 FakeNewsNet 数据集上，GAMC 在无监督方法中表现最佳（如 GossipCop 准确率 0.946），甚至接近或超越部分监督方法,需要新闻具有一定的传播量才能建模为图；早期传播阶段检测能力受限,https://ojs.aaai.org/index.php/AAAI/article/view/27788,,Proceedings of the AAAI Conference on Artificial Intelligence,"[AI generated] **中文标题：** GAMC：一种基于掩码图自编码器的无监督假新闻检测方法

**说明：**
- **GAMC** 作为方法名称保留不译，符合学术惯例。
- **Graph Autoencoder with Masking** 译为“基于掩码的图自编码器”或“掩码图自编码器”，准确传达了核心方法（图自编码器）及其关键技术（掩码操作）。
- **Unsupervised Method for",自编码器方法处理类社交网络图结构,assets/a855b33d/GAMC.png,"With the rise of social media, the spread of fake news has become a significant concern, potentially misleading public perceptions and impacting social stability. Although deep learning methods like CNNs, RNNs, and Transformer-based models like BERT have enhanced fake news detection. However, they primarily focus on content and do not consider social context during news propagation. Graph-based techniques have incorporated the social context but are limited by the need for large labeled datasets. To address these challenges, this paper introduces GAMC, an unsupervised fake news detection technique using the Graph Autoencoder with Masking and Contrastive learning. By leveraging both the context and content of news propagation as self-supervised signals, our method reduces the dependency on labeled datasets. Specifically, GAMC begins by applying data augmentation to the original news propagation graphs. Subsequently, these augmented graphs are encoded using a graph encoder and subsequently reconstructed via a graph decoder. Finally, a composite loss function that encompasses both reconstruction error and contrastive loss is designed. Firstly, it ensures the model can effectively capture the latent features, based on minimizing the discrepancy between reconstructed and original graph representations. Secondly, it aligns the representations of augmented graphs that originate from the same source. Experiments on the real-world dataset validate the effectiveness of our method.",anonymous,,TRUE,,2026/1/15 20:03,FALSE,FALSE,,,a855b33d
10.1145/3442442.3452328,How does truth evolve into fake news? An empirical study of fake news evolution,Mingfei Guo，Xiuying Chen，Juntao Li，Dongyan Zhao，Rui Yan,2021/6/3,Misinformation Analysis,而现有数据集多关注静态标注，缺乏对其假新闻演化过程的研究,给出了关注假新闻演化的数据集FNE，包含“真相-虚假新闻-演化虚假新闻”三元组,"1. 从 Snopes.com(一个辟谣网站) 抓取truth文章；
2. 通过其引文收集虚假新闻；
3. 利用网页存档平台（如 Archive Today）获取演化后版本；
4. 分析虚假信息技术分类（捏造、否认、混淆、歪曲四类、文本相似度、关键词、词性、情感等属性。",演化后虚假新闻与原始虚假新闻相似度更高，情感更客观积极，更难以被现有分类模型检测；虚假信息技术中以“捏造”为主；词性和关键词在演化中保持稳定。,数据来源依赖单一事实核查网站（Snopes），可能引入偏见；仅关注文本新闻，未涵盖图像、视频等多模态演变；,https://dl.acm.org/doi/10.1145/3442442.3452328,,Companion Proceedings of the Web Conference 2021,[AI generated] **中文标题：** 真相如何演变为虚假新闻？一项关于虚假新闻演化的实证研究,一个包含[原始新闻、假新闻、演化后的假新闻]三元组的数据集,assets/e2ca26be/FNE.png,"Automatically identifying fake news from the Internet is a challenging problem in deception detection tasks. Online news is modified constantly during its propagation, e.g., malicious users distort the original truth and make up fake news. However, the continuous evolution process would generate unprecedented fake news and cheat the original model. We present the Fake News Evolution (FNE) dataset: a new dataset tracking the fake news evolution process. Our dataset is composed of 950 paired data, each of which consists of articles representing the three significant phases of the evolution process, which are the truth, the fake news, and the evolved fake news. We observe the features during the evolution and they are the disinformation techniques, text similarity, top 10 keywords, classification accuracy, parts of speech, and sentiment properties.",anonymous,,TRUE,,2026/1/15 20:03,FALSE,FALSE,,,e2ca26be
10.1609/aaai.v39i1.32022,Learning Complex Heterogeneous Multimodal Fake News via Social Latent Network Inference,"Mingxin Li,Yuchen Zhang,Haowei Xu,Xianghua Li*,Chao Gao,Zhen Wang",2025/4/11,Misinformation Analysis,社交平台多元化导致新闻传播复杂、多模态，传统假新闻检测方法依赖显式传播关系（如转发），在抖音等平台难以直接获取，检测难度大。,提出“社交潜在网络推断Latent Network Inference”策略，无需真实传播关系，即可构建新闻间的潜在联系,"1. 社交潜在网络推断：基于Hawkes Process建模新闻影响力随时间变化，得到事件内部与事件间的影响强度，推断出潜在传播网络。
2. 异质图构建：节点均为新闻，边类型基于各种相同或相似属性（如作者、标题、时间等）构建。使用**注意力机制**动态融合不同边类型，生成统一的异质图表示（每个类型的边看做一个“头”，利用多头注意力方法）
3. 自监督多模态内容学习：损失函数：单模态增强（对同一模态进行掩码与重构）、跨模态对比学习（对齐不同模态（如文本与视频）的特征，通过对比学习拉近正样本、推开负样本）
4. 个性化图表示与分类：使用图Transformer Encoder融合图结构与模态特征，进行分类。",FakeSV和FVC数据集上准确率均超89%，较SOTA提升0.12%~4.39%；在Twitter/微博作为插件也提升明显（最高+10.71% F1）,依赖事件定义与时间序列假设，对实时性要求高；计算复杂度较高,https://ojs.aaai.org/index.php/AAAI/article/view/32022,,Proceedings of the AAAI Conference on Artificial Intelligence,"[AI generated] **中文标题：** 基于社交潜在网络推断的复杂异质多模态假新闻学习

**说明：** 此翻译力求准确、专业，符合中文科技论文的表达习惯：
1.  **核心术语处理**：
    *   ""Complex Heterogeneous Multimodal"" 译为“复杂异质多模态”，是领域内对数据/信息特性的标准表述。
    *   ""Social Latent Network Inference"" 译为“社交潜在网络推断”，其中“",通过社交潜在网络推断与自监督多模态学习检测复杂异质多模态假新闻的GNN方法,assets/7b9e4d80/HML.png,"With the diversification of online social platforms, news dissemination has become increasingly complex, heterogeneous, and multimodal, making the fake news detection task more challenging and crucial.
Previous works mainly focus on obtaining social relationships of news via retweets, limiting the accurate detection when real cascades are inaccessible. Given the proven assessment of the spreading influence of events, this paper proposes a method called HML (Complex Heterogeneous Multimodal Fake News Detection method via Latent Network Inference). Specifically, an improved social latent network inference strategy is designed to estimate the maximum likelihood of news influences under the same event. Meanwhile, a novel heterogeneous graph is built based on social attributes for multimodal news under different events. Further, to better aggregate the relationships among heterogeneous multimodal features, this paper proposes a self-supervised-based multimodal content learning strategy, to enhance, align, fuse and compare heterogeneous modal contents. Based above, a personalized heterogeneous graph representation learning is designed to classify fake news. Extensive experiments demonstrate that the proposed method outperforms the SOTA in real social media news datasets.",anonymous,,TRUE,,2026/1/15 20:03,FALSE,FALSE,,,7b9e4d80
10.1609/aaai.v38i20.30252,Harnessing Network Effect for Fake News Mitigation: Selecting Debunkers via Self-Imitation Learning,"Xiaofei Xu, Ke Deng, Michael Dann, Xiuzhen Zhang",2024/3/24,Misinformation Analysis,"Current approaches to multi-stage fake news mitigation often fail to address the episodic reward problem, where the effect of selecting an individual debunker cannot be measured until the campaign concludes. This sparse and delayed feedback limits the applicability of standard Reinforcement Learning (RL) in real-world social networks.
[翻译]
现有的多阶段假新闻治理方法往往未能解决片段式奖励问题，即选择单个辟谣者的效果只有在活动结束时才能衡量。这种稀疏且延迟的反馈限制了标准强化学习在现实社交网络中的适用性。","The authors propose NAGASIL, introducing two key enhancements to Self-Imitation Learning: 1) Negative Sampling, which leverages low-reward historical episodes to explicitly penalize poor debunker selections, and 2) State Augmentation, which enriches the observed state by integrating historical state-action sequences from the same campaign to address partial observability.
[翻译]
作者提出了NAGASIL，为自模仿学习引入了两个关键增强：1) 负采样，利用历史低奖励片段显式惩罚不良的辟谣者选择；2) 状态增强，通过融合同一活动中的历史状态-动作序列来丰富观测状态，以应对部分可观测性问题。","The debunker selection is formulated as a sequential decision-making problem under a budget constraint. A generative adversarial framework is employed, where a generator selects debunkers and a discriminator distinguishes between state-action pairs from high-reward historical episodes and those generated by the current policy. The generator is trained by integrating signals from the discriminator, an entropy regularizer for exploration, and a novel regularizer derived from a negative sampling model trained on low-reward episodes. This process yields an optimal generator capable of outputting an effective debunker selection policy.
[翻译]
辟谣者选择被建模为预算约束下的序列决策问题。采用生成对抗框架，其中生成器选择辟谣者，判别器区分来自高奖励历史片段的状态-动作对与当前策略生成的对。生成器的训练整合了来自判别器的信号、用于探索的熵正则项，以及一个从低奖励片段训练得到的负采样模型所衍生的新正则项。该过程最终产生一个能输出有效辟谣者选择策略的最优生成器。","Experiments conducted on both real-world (Facebook) and synthetic (Twitter) networks demonstrate that NAGASIL outperforms state-of-the-art fake news mitigation baselines and standard self-imitation learning methods across various budgets, stage lengths, and network densities.
[翻译]
在真实世界（Facebook）和合成（Twitter）网络上的实验表明，NAGASIL在各种预算、阶段长度和网络密度设置下，均优于先进的假新闻治理基线方法和标准自模仿学习方法。","The proposed method operates under the assumption that the veracity of news is pre-determined, necessitating integration with an external fake news detection system. Future research could explore adaptive propagation models and the dynamic nature of user behavior.
[翻译]
所提方法基于新闻真伪已知的假设运行，因此需要与外部假新闻检测系统结合。未来研究可探索自适应的传播模型和用户行为的动态特性。",https://ojs.aaai.org/index.php/AAAI/article/view/30252,https://github.com/xxfwin/NAGASIL,AAAI,"[AI generated] 中文标题：**利用网络效应抑制虚假新闻：基于自模仿学习的辟谣者选择策略**

（说明：该翻译在保持学术严谨性的同时，通过“抑制虚假新闻”准确传达“mitigation”的主动干预含义；“辟谣者选择策略”清晰体现了“selecting debunkers”的研究核心；副标题结构符合中文论文标题常见格式。）",一个辟谣者选择策略（MDP风格）的生成器,assets/78dbbf96/NAGASIL.png,"This study aims to minimize the influence of fake news on social networks by deploying debunkers to propagate true news. This is framed as a reinforcement learning problem, where, at each stage, one user is selected to propagate true news. A challenging issue is episodic reward where the ""net"" effect of selecting individual debunkers cannot be discerned from the interleaving information propagation on social networks, and only the collective effect from mitigation efforts can be observed. Existing Self-Imitation Learning (SIL) methods have shown promise in learning from episodic rewards, but are ill-suited to the real-world application of fake news mitigation because of their poor sample efficiency. To learn a more effective debunker selection policy for fake news mitigation, this study proposes NAGASIL - Negative sampling and state Augmented Generative Adversarial Self-Imitation Learning, which consists of two improvements geared towards fake news mitigation: learning from negative samples, and an augmented state representation to capture the ""real"" environment state by integrating the current observed state with the previous state-action pairs from the same campaign. Experiments on two social networks show that NAGASIL yields superior performance to standard GASIL and state-of-the-art fake news mitigation models.",anonymous,"[引用文]The work by Xu et al. (2024) marks a pivotal transition from merely recognizing patterns of disinformation to actively intervening to curtail its spread. By harnessing network effects within a reinforcement learning framework enhanced by self-imitative adversarial learning, their NAGASIL model transcends static pattern recognition. It implements a dynamic, goal-oriented policy learning process that provides actionable guidance for debunker selection strategies.
[翻译]
Xu等人（2024）的研究标志着一个关键的转变：从仅仅识别虚假信息模式，转向主动干预以遏制其传播。通过在一个由自模仿对抗学习增强的强化学习框架内利用网络效应，他们的NAGASIL模型超越了静态模式识别。它实现了一个动态的、目标导向的策略学习过程，为辟谣者选择策略提供了可行的指导。<br>[notes]1.这是一个对抗性学习框架，判别器希望好序列的置信度高，其他序列的置信度低，通过损失函数训练优化。生成器依据目标函数进行训练优化，目标函数包含三部分：判别器传来的对抗信号（置信度）、鼓励多样性的熵正则、负采样正则项（来自坏序列的距离（训练另一个模型以输出该值））。好经验和坏经验每轮通过奖励值V(τ) = -log(感染用户比例)获得。
2.每**轮**中每个**阶段**选择一个用户恢复并作为辟谣者，预算减去其成本，进行w个**时间步**的辟谣。每个阶段的预算用完后，进行每轮一次的梯度更新和好坏序列评选，然后进入下一轮",TRUE,,2026/1/15 20:03,FALSE,FALSE,,,78dbbf96
10.1609/icwsm.v19i1.35804,News Source Credibility Assessment: A Reddit Case Study,"Arash Amini, Yigit Ege Bayiz, Ashwin Ram, Radu Marculescu, and Ufuk Topcu",2025/6/7,Misinformation Analysis,"Driven by the proliferation of misinformation on social media, this work shifts focus from fact-checking individual news items to assessing the systemic credibility of news sources, addressing a critical gap in combating information pollution at its origin.
[翻译]
本研究受社交媒体虚假信息泛滥的驱动，将重点从核查单一新闻的真实性，转向评估新闻来源的系统性可信度，以应对从源头治理信息污染这一关键问题。","Its core innovation lies in constructing a weighted post-to-post network based on the semantic similarity of user comments. This network models latent socio-contextual linkages between submissions, which are then integrated via a Graph Convolutional Network (GCN) to enhance the binary classification of source credibility.
[翻译]
其核心创新在于基于用户评论的语义相似性构建了一个加权帖子间网络。该网络建模了帖子间潜在的社会语境关联，并通过图卷积网络（GCN）整合这些关联，以提升对新闻来源可信度的二分类性能。","The framework, CREDiBERT, first trains a bi-encoder on paired submissions about the same event to learn credibility-aware text embeddings. It then builds a novel graph where edge weights encode user reaction similarity via comments. Finally, a GCN fuses these textual and social signals for final classification.
[翻译]
该框架（CREDiBERT）首先在描述同一事件的成对帖子上训练一个双编码器，以学习具有可信度感知的文本嵌入。随后，它构建了一个新颖的图结构，其中边的权重通过评论编码了用户反应的相似性。最后，一个图卷积网络（GCN）融合了这些文本与社会信号以完成最终分类。","The model outperforms BERT-based baselines by 3% in F1-score for credibility assessment. Incorporating the user-interaction graph yields a further 8% gain, demonstrating the significant value of socially-grounded signals in evaluating information ecosystems.
[翻译]
该模型在可信度评估任务上的F1分数比基于BERT的基线模型高出3%。融入用户交互图后，性能进一步提升了8%，这证明了基于社交的感知信号在评估信息生态系统中的重要价值。","The method assesses source reputation rather than article veracity, leaving it blind to one-off falsehoods from otherwise credible outlets. Its performance is also tied to the bias present in the training data from specific online communities.
[翻译]
该方法评估的是来源声誉而非文章真实性，因此无法识别那些来自通常可信媒体的偶然性虚假信息。同时，其性能受限于来自特定网络社区的训练数据中存在的固有偏差。",https://ojs.aaai.org/index.php/ICWSM/article/view/35804,,ICWSM,[AI generated] 新闻来源可信度评估：一项基于Reddit的案例研究,通过帖子间的评论区相似性构建加权铁子网络，以求找到水军蛛丝马迹，进而确定新闻来源是否可信,assets/def176b4/CREDiBERT.png,"We present a transformer-based model for credibility assessment, CREDiBERT (CREDibility assessment using  Bi-directional Encoder Representations from Transformers), fine-tuned for Reddit submissions focusing on political discourse. We adopt a semi-supervised training approach for CREDiBERT, leveraging the community structure of Reddit. By encoding submission content using CREDiBERT and integrating it with a classification neural network, we improve the credibility assessment for Reddit submission by 3% in F1 score compared to existing methods. Additionally, we introduce a new version of the post-to-post network in Reddit that efficiently encodes user interactions to enhance the credibility assessment task by 8%  in the F1 score. We demonstrate CREDiBERT's applicability by evaluating the susceptibility of Reddit communities to different topics and assessing the credibility score of unseen sources.",anonymous,"[notes]关键设计在于通过评论区的评论相似性建模不同帖子间的潜在联系（加权帖子网络的边权重），在GCN中利用该联系来进行帖子的来源新闻的可信性二分类<br>[引用文]Amini et al. (2025) move beyond purely content-based pattern recognition, attempting instead to establish connections between posts and the credibility of news sources. Their CREDiBERT framework innovatively constructs a weighted post-to-post network from user comment similarities. This graph structure captures community-specific reaction patterns, which, when processed through a Graph Convolutional Network, significantly enhance the classification of news source credibility. This work underscores a paradigm shift: credibility assessment is beginning to focus on the patterns of information dissemination, rather than solely analyzing the specific content.
[翻译]
Amini等人（2025）的研究超越了单纯的基于内容的模式识别，转而尝试建立帖子与新闻来源可信度的联系。他们的CREDiBERT框架创新性地从用户评论相似性中构建了一个加权帖子间网络。该图结构捕获了社区的特定反应模式，这些模式通过图卷积网络处理后，显著增强了对新闻来源可信度的分类能力。这项工作强调了一个范式转变：可信度评估开始关注消息的传播模式，而不仅仅是分析具体内容。",TRUE,,2026/1/15 20:03,FALSE,FALSE,,,def176b4
10.1145/3627673.3679519,Let silence speak: Enhancing fake news detection with generated comments from large language models,"Qiong Nan,Qiang Sheng?,Juan Cao,Beizhe Hu,Danding Wang,Jintao Li",2024/10/21,Misinformation Analysis,"Comment-based fake news detection is hindered by the scarcity and skewed distribution of real user comments (e.g., in early stages or from “silent” users), leading to an incomplete and biased perception of public feedback.

[翻译]基于评论的虚假新闻检测受限于真实用户评论的稀缺性与分布偏差（例如在早期传播阶段或来自“沉默”用户），导致对公众反馈的感知不完整且存在偏差。",使用LLM补充评论特征，解决该领域评论数据不足和不全面的问题,"The GenFEND framework: (1) generates comments by prompting an LLM with 30 predefined user profiles (gender/age/education); (2) analyzes them via group-wise semantic averaging and diversity measurement across demographic views; (3) aggregates intra-view and inter-view features adaptively for final classification.

[翻译]GenFEND框架：(1) 通过为LLM提供30个预定义用户画像（性别/年龄/教育）来生成评论；(2) 通过分组语义平均和跨人口统计视图的多样性度量对其进行分析；(3) 自适应地聚合视图内和视图间的特征以进行最终分类。","GenFEND consistently boosts both content-only and comment-based detectors across datasets. Notably, LLM-generated comments provide effective signals for early detection and can outperform actual comments, especially in identifying fake news.

[翻译]GenFEND在多个数据集上持续提升了仅使用内容和使用评论的检测器性能。值得注意的是，LLM生成的评论为早期检测提供了有效信号，并且可以超越真实评论的效果，尤其在识别虚假新闻方面。","Limitations include dependence on LLM generation quality, limited considered user attributes, and higher computational cost. Future work may explore more nuanced user modeling, dynamic profile generation, and integration with real social graphs.

[翻译]局限性包括对LLM生成质量的依赖、所考虑用户属性的有限性以及较高的计算成本。未来工作可探索更细致的用户建模、动态画像生成以及与真实社交图谱的结合。",https://dl.acm.org/doi/10.1145/3627673.3679519,,CIKM '24,"[AI generated] **中文标题：** 让沉默发声：利用大语言模型生成评论增强虚假新闻检测

**说明：** 该翻译准确传达了原标题的核心思想，即通过生成“沉默用户”的评论来提升检测效果。“让沉默发声”这一表述生动且符合中文语境，整体风格专业、简洁，适用于学术论文标题。",“让沉默的用户发声——用LLM生成多样评论，补充评论特征，提升虚假新闻检测的覆盖力和早期性能。”,assets/f607b9b7/GenFEND.png,"Fake news detection plays a crucial role in protecting social media users and maintaining a healthy news ecosystem. Among existing works, comment-based fake news detection methods are empirically shown as promising because comments could reflect users' opinions, stances, and emotions and deepen models' understanding of fake news. Unfortunately, due to exposure bias and users' different willingness to comment, it is not easy to obtain diverse comments in reality, especially for early detection scenarios. Without obtaining the comments from the ""silent'' users, the perceived opinions may be incomplete, subsequently affecting news veracity judgment. In this paper, we explore the possibility of finding an alternative source of comments to guarantee the availability of diverse comments, especially those from silent users. Specifically, we propose to adopt large language models (LLMs) as a user simulator and comment generator, and design GenFEND, a generated feedback-enhanced detection framework, which generates comments by prompting LLMs with diverse user profiles and aggregating generated comments from multiple subpopulation groups. Experiments demonstrate the effectiveness of GenFEND and further analysis shows that the generated comments cover more diverse users and could even be more effective than actual comments.",anonymous,"[引用文]Within the task of False Content Analysis, a key bottleneck is the scarcity of early-stage comments and the absence of opinions from silent users. The GenFEND framework (Nan et al., 2024) addresses this by using Large Language Models (LLMs) to supplement these missing comments. Instead of passively relying on sparse real comments, their approach actively generates a rich set of synthetic comments conditioned on diverse user profiles (e.g., gender, age, education level). This method effectively performs data augmentation in the social comment space, providing a stable and diverse informational supplement. This helps models establish a more complete perceptual foundation for veracity judgment and has proven to be highly effective for early fake news detection.
[翻译]
在虚假内容分析任务中，一个关键瓶颈是早期评论的稀缺性和沉默用户意见的缺失。GenFEND框架 (Nan et al., 2024) 通过使用大语言模型来补充这部分缺失的评论，从而解决了这一问题。该方法不再被动地依赖稀疏的真实评论，而是主动生成一组以多样化用户画像（如性别、年龄、教育程度）为条件的丰富合成评论。该方法有效地在社交评论空间进行了数据增强，提供了一个稳定且多样化的信息补充。这有助于模型为真实性判断建立更完整的感知基础，并被证明对早期虚假新闻检测非常有效。",TRUE,,2026/1/15 20:03,FALSE,FALSE,,,f607b9b7
10.1007/s11280-022-01116-0,A text and GNN based controversy detection method on social media,"Samy Benslimane, Jér?me Azé, Sandra Bringay, Maximilien Servajean, Caroline Mollevi",2023/3/1,Controversy Analysis,"Existing controversy detection approaches often treat structural patterns and semantic content in isolation or rely solely on post-reply trees, neglecting the critical role of user entities and their interaction dynamics in driving social polarization.
[翻译] 现有的争议检测方法通常将结构模式和语义内容割裂处理，或单纯依赖帖子回复树结构，忽视了用户实体及其交互动态在驱动社会极化中的关键作用。","The study shifts focus from message-level dependencies to user-centric interaction graphs. A key innovation lies in aggregating individual comments into unified user features, which allows the model to encode network-level information and actor stances simultaneously within the graph structure.
[翻译] 该研究将焦点从消息级依赖关系转移到了以用户为中心的交互图上。其主要创新在于将分散的评论聚合为统一的用户特征，这使得模型能够在图结构中同时编码网络层面的信息和行动者的立场。","The framework constructs a user graph where node features are initialized with BERT-encoded textual features aggregated from user historical comments. Subsequently, two GNN methods—Hierarchical Representation Learning via differentiable pooling (HRL-GCN) or Attention-based Representation Learning (ARL-GAT)—are employed to capture structural patterns, ultimately performing graph-level binary classification.
[翻译] 该框架构建了一个用户图，其中节点特征初始化为从用户历史评论中聚合的BERT编码文本特征。随后，利用通过可微池化实现的分层表示学习（HRL-GCN）或基于注意力的表示学习（ARL-GAT）两种GNN方法来捕捉结构模式，最终完成图级二分类。","Empirical evaluations on Reddit and Twitter datasets demonstrate that the hierarchical pooling strategy (HRL-GCN) achieves superior performance by effectively capturing community-level structures, validating that combining aggregated user semantics with interaction topology significantly outperforms structure-only baselines.
[翻译] 在Reddit和Twitter数据集上的实证评估表明，分层池化策略（HRL-GCN）通过有效捕捉社区级结构实现了更优的性能，证实了结合聚合的用户语义与交互拓扑结构显著优于仅依赖基线的结构。","Standard pre-trained language models perform suboptimally on noisy Twitter data without domain-specific fine-tuning. Additionally, the approach is currently limited to homogeneous graphs, suggesting that future work could explore heterogeneous graph modeling to integrate multiple interaction types.
[翻译] 标准预训练语言模型在未经领域微调的情况下在噪声较大的Twitter数据上表现欠佳，以及仅限于同构图，未来可以探索异构图建模以整合多种交互类型。",https://doi.org/10.1007/s11280-022-01116-0,https://github.com/gvrkiran/controversy-detection,World Wide Web,[AI generated] **中文标题：** 一种基于文本与图神经网络的社交媒体争议检测方法,"To identify controversial topics from discussion inputs, the method aggregates historical comments into user node features to encode intrinsic user and network attributes, and subsequently applies GNN methods on the user-centric social network to achieve binary classification.
[翻译] 在争议话题识别中，输入是一段讨论，为了编码用户本身特征和用户网络特征，将用户历史评论聚合为用户节点特征，在以用户为节点的社交网络上，进行GNN方法，实现二分类。",assets/3d0a31c8/HRL-GCN.png,"Expressed opinions on social media frequently cause a controversy. Controversial content refers to content that attracts different opinions and interrogations, implying interaction between communities. Its automatic identification remains a challenging task. Most of the existing approaches rely on the graph structure of discussion and/or the content of messages but did not deeply explore the recent advances on Graph Neural Network (gnn) to predict if a discussion is controversial or not. This paper aims to combine both user interactions present in the graph structure of a discussion and the discussion text features to detect controversy. We rely on sampling techniques to reduce the size of large graphs and augment the graph training set if needed. Our proposed approach relies then on gnn techniques to encode the initial (or sampled) graph in an embedding vector before performing a graph classification task. We propose two controversy detection strategies. The first one is based on a hierarchical graph representation learning to take advantage of hierarchical relationships that could exist between users. The second one is based on the attention mechanism, which allows each user node to give more or less importance to its neighbors when computing node embeddings. We present different experiments conducted with data sources collected from both Reddit and Twitter to show the applicability of our approach to different social networks. Conducted experiments show the positive impact of combining textual features and structural information in terms of performance and accuracy.",anonymous,"[引用文]To better capture the dynamics of social polarization in the field of controversial content identification, Benslimane et al. (2023) shift from content-based analysis to a user-centric perspective. They argue that the structural relationships between users and the users' inherent attributes are pivotal for understanding controversy. Unlike traditional methods that treat posts as isolated nodes, their approach constructs a user interaction graph where node representations are derived by aggregating the semantic content of each user’s historical comments. This design encodes complex network information and individual stances into a unified feature space. By employing GNN methods such as Hierarchical Graph Representation Learning (HRL-GCN), the model performs multi-layer aggregation on the graph to capture high-level community structures, demonstrating that the fusion of user features with social interaction patterns provides an effective mechanism for controversy identification in social media.
[翻译] 为了更好地在争议内容识别领域捕捉社会极化的动态，Benslimane等人 (2023) 的方法从基于内容的分析转变为以用户为中心视角。他们认为，用户之间的结构关系以及用户本身的属性对于理解争议至关重要。与将帖子视为孤立节点的传统方法不同，他们的方法构建了一个用户交互图，其中节点表示通过聚合每个用户历史评论的语义内容得到。这种设计将复杂的网络信息和个人立场编码到了统一的特征空间中。通过采用分层图表示学习（HRL-GCN）等GNN的方法，该模型对图进行多层聚合以捕捉高层级的社区结构，证明了用户特征与社交交互模式的融合为识别社交媒体中的争议识别提供了一种有效的机制。",TRUE,unread,2026/1/29 10:07,FALSE,FALSE,,,3d0a31c8
